import numpy as np
import os
import sys
import pickle
import copy
import tarfile


from process_results_qm7b_2 import save_learning_curves, save_scatter_data

from hashlib import md5


# def get_res_summary(resdata):
    
#     res_summary = {}
    
#     # get the subsets
#     subsets = []
#     for res in resdata:
#         sub = res['subset']
#         if sub not in res_summary:
#             res_summary[sub] = {}
#             subsets.append(sub)
    
#     for sub in subsets:
        
#         train_sizes = []
#         mae_test_av = []
#         mae_train_av = []
#         std_err_test = []
#         uuids = []
#         for res in resdata:
#             if res['subset'] == sub:
#                 train_sizes.append(res['results'][0]['n_train'])
#                 mae_test_av.append(res['mae_test_av'])
#                 mae_train_av.append(res['mae_train_av'])
#                 std_err_test.append(res['std_err_test'])
#                 uuids.append(res['uuid'])
                
#         train_sizes = np.asarray(train_sizes)
#         mae_test_av = np.asarray(mae_test_av)
#         mae_train_av = np.asarray(mae_train_av)
#         std_err_test = np.asarray(std_err_test)
#         uuids = np.asarray(uuids)
        
#         sorting = np.argsort(train_sizes)
#         res_summary[sub] = {'train_sizes': train_sizes[sorting], 
#                             'mae_av': mae_test_av[sorting], 
#                             'mae_train_av': mae_train_av[sorting], 
#                             'std_err': std_err_test[sorting],
#                             'uuids': uuids[sorting]}
        
#     return (res_summary, subsets)


# def get_data_for_scatter_plots(res_data, subset_list, train_sizes, iter_index):

#     scatter_data = {}

#     for i, sub in enumerate(subset_list):
#         data_tmp = {}
#         for n_train in train_sizes[i]:
#             for res in res_data:
#                 if (res['subset'] == sub ) & (res['results'][iter_index]['n_train'] == n_train):
#                     res_sub = res['results'][iter_index]
#                     data_tmp[n_train] = {'Y_train': res_sub['Y_train'], 'Y_test': res_sub['Y_test'],
#                                          'Y_pred_train': res_sub['Y_pred_train'],
#                                          'Y_pred_test': res_sub['Y_pred_test'],
#                                          'uuid': res['uuid']}
#         scatter_data[sub] = data_tmp

#     return scatter_data


def get_averaged_mae(res, n_iter):

    mae_list = []
    for i in range(n_iter):
        mae_list.append(res[i]['mae_test'])
    mae_test_av = np.mean(mae_list)
    std_err = np.std(mae_list)

    return (mae_test_av, std_err)


def get_res_learning_curves(res_list, subset, n_iter, train_sizes):

    if subset == -1:
        res_sum = {}
        for sub_test in [0, 1, 2]:
            mae_tmp = []
            std_err_tmp = []
            for n_train in train_sizes:
                mae_av, std_err = get_averaged_mae(res_list[n_train][sub_test]['results'], n_iter)
                mae_tmp.append(mae_av)
                std_err_tmp.append(std_err_tmp)
            res_sum[sub_test] = {'mae_av': mae_tmp, 'std_err': std_err_tmp}
    else:
        mae_tmp = []
        std_err_tmp = []
        for n_train in train_sizes:
            mae_av, std_err = get_averaged_mae(res_list[n_train]['results'], n_iter)
            mae_tmp.append(mae_av)
            std_err_tmp.append(std_err_tmp)
        res_sum = {'mae_av': mae_tmp, 'std_err': std_err_tmp}
                
    return res_sum


def get_res_scatter_data(res_list, subset, iter_ind, train_sizes):

    if subset == -1:
        res_sum = {}
        for sub_test in [0, 1, 2]:
            y_ref = {}
            y_pred = {}
            test_ind = {}
            train_ind = {}
            for n_train in train_sizes:
                y_ref[n_train] = res_list[n_train][sub_test]['results'][iter_ind]['Y_test']
                y_pred[n_train] = res_list[n_train][sub_test]['results'][iter_ind]['Y_pred_test']
                train_ind[n_train] = res_list[n_train][sub_test]['results'][iter_ind]['train_ind']
                test_ind[n_train] = res_list[n_train][sub_test]['results'][iter_ind]['test_ind']
            res_sum[sub_test] = {'Y_ref': y_ref, 'Y_pred': y_pred,
                                 'train_ind': train_ind, 'test_ind': test_ind}
    else:
        y_ref = {}
        y_pred = {}
        test_ind = {}
        train_ind = {}
        for n_train in train_sizes:
            y_ref[n_train] = res_list[n_train]['results'][iter_ind]['Y_test']
            y_pred[n_train] = res_list[n_train]['results'][iter_ind]['Y_pred_test']
            train_ind[n_train] = res_list[n_train]['results'][iter_ind]['train_ind']
            test_ind[n_train] = res_list[n_train]['results'][iter_ind]['test_ind']
        res_sum = {'Y_ref': y_ref, 'Y_pred': y_pred,
                   'train_ind': train_ind, 'test_ind': test_ind}

    return res_sum
                

def main(inputdir, inputfile, outputdir, outputfile, iter_index):

    iter_index = int(iter_index)

    subsets = [-1, 0, 1, 2]

    prop_list = ['G', 'H', 'L']
    # prop_list = ['G', 'H']
    # prop_list = ['L']
    R = 'slatm'
    ktype = 'laplacian'
    sigmas = {-1: 256.0, 0: 256.0, 1: 256.0, 2: 256.0}
    lam_exp = -12
    n_iter = 10

    train_sizes_dict = {-1: [1000, 2000, 4000, 8000, 16000, 32000, 64000],
                        -0: [1000, 2000, 4000, 8000, 16000, 32000],
                        1: [1000, 2000, 4000, 8000, 16000, 32000],
                        2: [1000, 2000, 4000, 8000, 16000]}

    for prop in prop_list:
        
        print('Fetching results for property {prop}.')
        
        for sub in subsets:
            
            print(f'\tFetching results for subset {sub}.')
            
            sig = sigmas[sub]
            res_tmp = {}
            for n_train in train_sizes_dict[sub]:
                hash_str = f'qm9 loop {prop} {sub} {R} {ktype} {sig} {lam_exp} {n_train}'
                print(hash_str)
                res_hash = md5(bytes(hash_str, 'utf-8')).hexdigest()
                with open(f'{inputdir}{inputfile}_{res_hash}.pkl', 'rb') as inf:
                    res_tmp[n_train] = pickle.load(inf)
                    
            res_sum_lc = get_res_learning_curves(res_tmp, sub, n_iter, train_sizes_dict[sub])
            save_learning_curves(res_sum_lc, prop, sub, R, ktype, sig, lam_exp, res_hash,
                                 train_sizes_dict[sub], outputdir, outputfile)
            
            res_sum_scatter = get_res_scatter_data(res_tmp, sub, iter_index, train_sizes_dict[sub])
            save_scatter_data(res_sum_scatter, prop, sub, R, ktype, sig, lam_exp, res_hash,
                              outputdir, outputfile)


    return


if __name__ == '__main__':

    main( * sys.argv[1:])
